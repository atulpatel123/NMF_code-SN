{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DNMF**"
      ],
      "metadata": {
        "id": "NI4zr12sYXiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1-LHlDHYB-M"
      },
      "outputs": [],
      "source": [
        "#final deepnmf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "source_data = user_item_matrix_books\n",
        "target_data = user_item_matrix_dvds\n",
        "\n",
        "train_data, test_data = train_test_split(target_data, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "source_data = scaler.fit_transform(source_data)\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n",
        "\n",
        "def build_deepNMF_model(input_dim, latent_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(latent_dim, activation='relu')(input_layer)\n",
        "    encoded = Dropout(0.5)(encoded)\n",
        "    encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return autoencoder, encoder\n",
        "\n",
        "input_dim_source = source_data.shape[1]\n",
        "input_dim_target = train_data.shape[1]\n",
        "latent_dim = 30  # Increased latent dimension\n",
        "\n",
        "autoencoder_source, encoder_source = build_deepNMF_model(input_dim_source, latent_dim)\n",
        "autoencoder_target, encoder_target = build_deepNMF_model(input_dim_target, latent_dim)\n",
        "\n",
        "autoencoder_source.fit(source_data, source_data, epochs=100, batch_size=20, shuffle=True)  # Increased epochs and batch size\n",
        "\n",
        "# Transfer learning: Initialize target domain encoder with matching weights from the source domain encoder\n",
        "source_weights = encoder_source.get_weights()\n",
        "target_weights = encoder_target.get_weights()\n",
        "\n",
        "# The weight matrices are stored in the form [weights, biases]\n",
        "# Partially update the target encoder weights with the source encoder weights\n",
        "target_weights[0][:input_dim_source] = source_weights[0][:input_dim_target]\n",
        "target_weights[1] = source_weights[1]\n",
        "\n",
        "encoder_target.set_weights(target_weights)\n",
        "\n",
        "autoencoder_target.fit(train_data, train_data, epochs=100, batch_size=20, shuffle=True)  # Increased epochs and batch size\n",
        "\n",
        "test_data_pred = autoencoder_target.predict(test_data)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(test_data, test_data_pred))\n",
        "mae = mean_absolute_error(test_data, test_data_pred)\n",
        "\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'MAE: {mae}')\n",
        "\n",
        "threshold = 0.5\n",
        "test_data_bin = (test_data > threshold).astype(int)\n",
        "test_data_pred_bin = (test_data_pred > threshold).astype(int)\n",
        "\n",
        "precision = precision_score(test_data_bin.flatten(), test_data_pred_bin.flatten(), average='macro')\n",
        "recall = recall_score(test_data_bin.flatten(), test_data_pred_bin.flatten(), average='macro')\n",
        "f1 = f1_score(test_data_bin.flatten(), test_data_pred_bin.flatten(), average='macro')\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import pandas as pd\n",
        "\n",
        "# Example data for source and target domains\n",
        "np.random.seed(42)\n",
        "\n",
        "# Placeholder for user-item matrices (replace with actual matrices)\n",
        "source_data = user_item_matrix_books\n",
        "target_data = user_item_matrix_dvds\n",
        "\n",
        "# Data preparation\n",
        "scaler = StandardScaler()\n",
        "source_data = scaler.fit_transform(source_data)\n",
        "target_data = scaler.fit_transform(target_data)\n",
        "\n",
        "# Function to build the DeepNMF model\n",
        "def build_deepNMF_model(input_dim, latent_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(latent_dim, activation='relu')(input_layer)\n",
        "    encoded = Dropout(0.5)(encoded)\n",
        "    encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return autoencoder, encoder\n",
        "\n",
        "# Function to perform 5-fold cross-validation with verbose outputs\n",
        "def cross_validate_deepNMF_verbose(train_data, k=5, latent_dim=30, epochs=100, batch_size=20):\n",
        "    # Ensure the data is a NumPy array\n",
        "    if isinstance(train_data, pd.DataFrame):\n",
        "        train_data = train_data.to_numpy()\n",
        "\n",
        "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    metrics = {'rmse': [], 'mae': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_data)):\n",
        "        print(f\"\\n===== Starting Fold {fold + 1} =====\")\n",
        "\n",
        "        # Split data into training and validation sets for the current fold\n",
        "        train_fold = train_data[train_idx]\n",
        "        val_fold = train_data[val_idx]\n",
        "\n",
        "        # Build the model for the current fold\n",
        "        input_dim = train_fold.shape[1]\n",
        "        autoencoder, encoder = build_deepNMF_model(input_dim, latent_dim)\n",
        "\n",
        "        # Train the model on the training fold\n",
        "        autoencoder.fit(train_fold, train_fold, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=0)\n",
        "\n",
        "        # Predict on the validation fold\n",
        "        val_pred = autoencoder.predict(val_fold)\n",
        "\n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(val_fold, val_pred))\n",
        "        mae = mean_absolute_error(val_fold, val_pred)\n",
        "\n",
        "        threshold = 0.5\n",
        "        val_fold_bin = (val_fold > threshold).astype(int)\n",
        "        val_pred_bin = (val_pred > threshold).astype(int)\n",
        "\n",
        "        precision = precision_score(val_fold_bin.flatten(), val_pred_bin.flatten(), average='macro', zero_division=0)\n",
        "        recall = recall_score(val_fold_bin.flatten(), val_pred_bin.flatten(), average='macro', zero_division=0)\n",
        "        f1 = f1_score(val_fold_bin.flatten(), val_pred_bin.flatten(), average='macro', zero_division=0)\n",
        "\n",
        "        # Store the metrics\n",
        "        metrics['rmse'].append(rmse)\n",
        "        metrics['mae'].append(mae)\n",
        "        metrics['precision'].append(precision)\n",
        "        metrics['recall'].append(recall)\n",
        "        metrics['f1'].append(f1)\n",
        "\n",
        "        # Print results for the current fold\n",
        "        print(f\"Fold {fold + 1} Results:\")\n",
        "        print(f\"  RMSE: {rmse:.4f}\")\n",
        "        print(f\"  MAE: {mae:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Print average metrics across all folds\n",
        "    print(\"\\n===== Cross-Validation Results =====\")\n",
        "    for metric, values in metrics.items():\n",
        "        print(f\"{metric.upper()} (average over {k} folds): {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
        "\n",
        "# Apply cross-validation on the target domain data\n",
        "cross_validate_deepNMF_verbose(target_data, k=5, latent_dim=30, epochs=100, batch_size=20)\n"
      ],
      "metadata": {
        "id": "MS4diX90YjgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ONMF**"
      ],
      "metadata": {
        "id": "jDOJevEQYqGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define Orthogonal NMF model with modifications\n",
        "class OrthogonalNMF(tf.keras.Model):\n",
        "    def __init__(self, num_users, num_items, num_features, reg_lambda=0.001, ortho_lambda=0.01):\n",
        "        super(OrthogonalNMF, self).__init__()\n",
        "        self.user_features = self.add_weight(\n",
        "            shape=(num_users, num_features),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='user_features'\n",
        "        )\n",
        "        self.item_features = self.add_weight(\n",
        "            shape=(num_items, num_features),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='item_features'\n",
        "        )\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.ortho_lambda = ortho_lambda\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_idx, item_idx = inputs\n",
        "        user_features = tf.gather(self.user_features, user_idx)\n",
        "        item_features = tf.gather(self.item_features, item_idx)\n",
        "        ratings = tf.reduce_sum(user_features * item_features, axis=1)\n",
        "        return ratings\n",
        "\n",
        "    def compute_loss(self, true_ratings, pred_ratings):\n",
        "        mse_loss = tf.reduce_mean(tf.square(true_ratings - pred_ratings))\n",
        "        reg_loss = self.reg_lambda * (tf.nn.l2_loss(self.user_features) + tf.nn.l2_loss(self.item_features))\n",
        "\n",
        "        # Orthogonality constraint applied to features (latent dimensions)\n",
        "        user_ortho_loss = tf.reduce_sum(\n",
        "            tf.square(tf.matmul(self.user_features, self.user_features, transpose_a=True) - tf.eye(self.user_features.shape[1]))\n",
        "        )\n",
        "        item_ortho_loss = tf.reduce_sum(\n",
        "            tf.square(tf.matmul(self.item_features, self.item_features, transpose_a=True) - tf.eye(self.item_features.shape[1]))\n",
        "        )\n",
        "\n",
        "        ortho_loss = self.ortho_lambda * (user_ortho_loss + item_ortho_loss)\n",
        "        return mse_loss + reg_loss + ortho_loss\n",
        "\n",
        "# Normalize ratings\n",
        "def normalize_ratings(ratings):\n",
        "    mean_rating = np.mean(ratings[ratings > 0])\n",
        "    std_rating = np.std(ratings[ratings > 0])\n",
        "    normalized_ratings = np.zeros_like(ratings)\n",
        "    normalized_ratings[ratings > 0] = (ratings[ratings > 0] - mean_rating) / std_rating\n",
        "    return normalized_ratings, mean_rating, std_rating\n",
        "\n",
        "def denormalize_ratings(norm_ratings, mean_rating, std_rating):\n",
        "    return norm_ratings * std_rating + mean_rating\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, data, mean_rating, std_rating):\n",
        "    user_idx, item_idx = np.nonzero(data)\n",
        "    true_ratings = data[user_idx, item_idx].astype(np.float32)\n",
        "\n",
        "    # Normalize true ratings\n",
        "    true_ratings_norm = (true_ratings - mean_rating) / std_rating\n",
        "\n",
        "    pred_ratings_norm = model((user_idx, item_idx)).numpy()\n",
        "\n",
        "    # Denormalize predicted ratings\n",
        "    pred_ratings = denormalize_ratings(pred_ratings_norm, mean_rating, std_rating)\n",
        "\n",
        "    # Clip predicted ratings to valid range\n",
        "    min_rating = np.min(true_ratings)\n",
        "    max_rating = np.max(true_ratings)\n",
        "    pred_ratings = np.clip(pred_ratings, min_rating, max_rating)\n",
        "\n",
        "    # Compute RMSE and MAE\n",
        "    rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings))\n",
        "    mae = mean_absolute_error(true_ratings, pred_ratings)\n",
        "\n",
        "    # Binarize the ratings for classification metrics\n",
        "    threshold = np.mean(true_ratings)\n",
        "    true_binary = (true_ratings >= threshold).astype(int)\n",
        "    pred_binary = (pred_ratings >= threshold).astype(int)\n",
        "\n",
        "    precision = precision_score(true_binary, pred_binary, zero_division=0)\n",
        "    recall = recall_score(true_binary, pred_binary, zero_division=0)\n",
        "    f1 = f1_score(true_binary, pred_binary, zero_division=0)\n",
        "\n",
        "    return rmse, mae, precision, recall, f1\n",
        "\n",
        "# Cross-validation training\n",
        "def train_model_cross_val(ratings, num_folds=5, epochs=100, learning_rate=0.001, num_features=50, reg_lambda=0.001, ortho_lambda=0.01):\n",
        "    num_users, num_items = ratings.shape\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(ratings)):\n",
        "        print(f\"\\nStarting Fold {fold + 1}...\")\n",
        "        train_data, test_data = ratings[train_idx], ratings[test_idx]\n",
        "\n",
        "        model = OrthogonalNMF(num_users, num_items, num_features, reg_lambda, ortho_lambda)\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        user_idx, item_idx = np.nonzero(train_data)\n",
        "        true_ratings = train_data[user_idx, item_idx].astype(np.float32)\n",
        "\n",
        "        # Normalize ratings\n",
        "        mean_rating = np.mean(true_ratings)\n",
        "        std_rating = np.std(true_ratings)\n",
        "        true_ratings_norm = (true_ratings - mean_rating) / std_rating\n",
        "\n",
        "        # Create a dataset pipeline\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((user_idx, item_idx, true_ratings_norm))\n",
        "        dataset = dataset.shuffle(buffer_size=100000).batch(512)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "            for batch_user_idx, batch_item_idx, batch_ratings in dataset:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    pred_ratings = model((batch_user_idx, batch_item_idx))\n",
        "                    loss = model.compute_loss(batch_ratings, pred_ratings)\n",
        "                gradients = tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "                total_loss += loss.numpy()\n",
        "                num_batches += 1\n",
        "            avg_loss = total_loss / num_batches\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Evaluate the model\n",
        "        rmse, mae, precision, recall, f1 = evaluate_model(model, test_data, mean_rating, std_rating)\n",
        "        print(f\"Fold {fold + 1} Results -> RMSE: {rmse:.4f}, MAE: {mae:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "        fold_results.append((rmse, mae, precision, recall, f1))\n",
        "\n",
        "    # Compute average metrics across folds\n",
        "    fold_results = np.array(fold_results)\n",
        "    avg_metrics = np.mean(fold_results, axis=0)\n",
        "    std_metrics = np.std(fold_results, axis=0)\n",
        "\n",
        "    print(\"\\nCross-Validation Results:\")\n",
        "    print(f\"RMSE: {avg_metrics[0]:.4f} ± {std_metrics[0]:.4f}\")\n",
        "    print(f\"MAE: {avg_metrics[1]:.4f} ± {std_metrics[1]:.4f}\")\n",
        "    print(f\"Precision: {avg_metrics[2]:.4f} ± {std_metrics[2]:.4f}\")\n",
        "    print(f\"Recall: {avg_metrics[3]:.4f} ± {std_metrics[3]:.4f}\")\n",
        "    print(f\"F1-score: {avg_metrics[4]:.4f} ± {std_metrics[4]:.4f}\")\n",
        "\n",
        "    return avg_metrics, std_metrics\n",
        "\n",
        "# Align users across datasets\n",
        "if isinstance(user_item_matrix_books, pd.DataFrame):\n",
        "    user_item_matrix_books = user_item_matrix_books.values\n",
        "if isinstance(user_item_matrix_dvds, pd.DataFrame):\n",
        "    user_item_matrix_dvds = user_item_matrix_dvds.values\n",
        "\n",
        "num_users_books, num_items_books = user_item_matrix_books.shape\n",
        "num_users_dvds, num_items_dvds = user_item_matrix_dvds.shape\n",
        "\n",
        "if num_users_books != num_users_dvds:\n",
        "    print(\"Aligning users between Books and DVDs datasets...\")\n",
        "    common_users = min(num_users_books, num_users_dvds)\n",
        "    user_item_matrix_books = user_item_matrix_books[:common_users, :]\n",
        "    user_item_matrix_dvds = user_item_matrix_dvds[:common_users, :]\n",
        "\n",
        "num_users, num_items_books = user_item_matrix_books.shape\n",
        "_, num_items_dvds = user_item_matrix_dvds.shape\n",
        "num_features = 50\n",
        "\n",
        "# Pretrain on Books\n",
        "print(\"Training on Books dataset for transfer learning...\")\n",
        "model_books = OrthogonalNMF(num_users, num_items_books, num_features, reg_lambda=0.001, ortho_lambda=0.01)\n",
        "optimizer_books = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "user_idx_books, item_idx_books = np.nonzero(user_item_matrix_books)\n",
        "true_ratings_books = user_item_matrix_books[user_idx_books, item_idx_books].astype(np.float32)\n",
        "mean_rating_books, std_rating_books = np.mean(true_ratings_books), np.std(true_ratings_books)\n",
        "true_ratings_books_norm = (true_ratings_books - mean_rating_books) / std_rating_books\n",
        "\n",
        "dataset_books = tf.data.Dataset.from_tensor_slices((user_idx_books, item_idx_books, true_ratings_books_norm))\n",
        "dataset_books = dataset_books.shuffle(buffer_size=100000).batch(512)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for batch_user_idx, batch_item_idx, batch_ratings in dataset_books:\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_ratings = model_books((batch_user_idx, batch_item_idx))\n",
        "            loss = model_books.compute_loss(batch_ratings, pred_ratings)\n",
        "        gradients = tape.gradient(loss, model_books.trainable_variables)\n",
        "        optimizer_books.apply_gradients(zip(gradients, model_books.trainable_variables))\n",
        "        total_loss += loss.numpy()\n",
        "    print(f\"Books Pretraining - Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "pretrained_user_features = model_books.user_features.numpy()\n",
        "\n",
        "# Train on DVDs using transfer learning\n",
        "print(\"\\nStarting training on DVDs dataset using transfer learning...\")\n",
        "model_dvds = OrthogonalNMF(num_users, num_items_dvds, num_features, reg_lambda=0.001, ortho_lambda=0.01)\n",
        "model_dvds.user_features.assign(pretrained_user_features)\n",
        "\n",
        "# Perform cross-validation on DVDs\n",
        "avg_metrics, std_metrics = train_model_cross_val(\n",
        "    user_item_matrix_dvds,\n",
        "    num_folds=5,\n",
        "    epochs=100,\n",
        "    learning_rate=0.001,\n",
        "    num_features=num_features,\n",
        "    reg_lambda=0.001,\n",
        "    ortho_lambda=0.01\n",
        ")\n"
      ],
      "metadata": {
        "id": "_JHYvqIXYtXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SNMF**"
      ],
      "metadata": {
        "id": "1dwBf8EKZEEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define NMF model with biases\n",
        "class StandardNMF(tf.keras.Model):\n",
        "    def __init__(self, num_users, num_items, num_features, regularization=0.01):\n",
        "        super(StandardNMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.num_features = num_features\n",
        "        self.regularization = regularization\n",
        "        self.user_features = self.add_weight(\n",
        "            shape=(num_users, num_features),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='user_features'\n",
        "        )\n",
        "        self.item_features = self.add_weight(\n",
        "            shape=(num_items, num_features),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='item_features'\n",
        "        )\n",
        "        self.user_bias = self.add_weight(\n",
        "            shape=(num_users,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='user_bias'\n",
        "        )\n",
        "        self.item_bias = self.add_weight(\n",
        "            shape=(num_items,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='item_bias'\n",
        "        )\n",
        "        self.global_bias = self.add_weight(\n",
        "            shape=(),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='global_bias'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_idx, item_idx = inputs\n",
        "        user_features = tf.gather(self.user_features, user_idx)\n",
        "        item_features = tf.gather(self.item_features, item_idx)\n",
        "        user_bias = tf.gather(self.user_bias, user_idx)\n",
        "        item_bias = tf.gather(self.item_bias, item_idx)\n",
        "        dot_product = tf.reduce_sum(user_features * item_features, axis=1)\n",
        "        return dot_product + user_bias + item_bias + self.global_bias\n",
        "\n",
        "    def compute_loss(self, true_ratings, pred_ratings):\n",
        "        base_loss = tf.reduce_mean(tf.square(true_ratings - pred_ratings))\n",
        "        reg_loss = self.regularization * (\n",
        "            tf.nn.l2_loss(self.user_features) + tf.nn.l2_loss(self.item_features) +\n",
        "            tf.nn.l2_loss(self.user_bias) + tf.nn.l2_loss(self.item_bias)\n",
        "        )\n",
        "        return base_loss + reg_loss\n"
      ],
      "metadata": {
        "id": "fKXnQvw-ZHHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NSNMF**"
      ],
      "metadata": {
        "id": "SV-CmsANZWJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply non-smooth NMF\n",
        "def apply_non_smooth_nmf(data, n_components=50, max_iter=1000, alpha=0.0, l1_ratio=0.0, init_W=None):\n",
        "    model = NMF(\n",
        "        n_components=n_components,\n",
        "        init='custom' if init_W is not None else 'nndsvda',\n",
        "        solver='cd',\n",
        "        beta_loss='frobenius',\n",
        "        max_iter=max_iter,\n",
        "        alpha_W=alpha,\n",
        "        alpha_H=alpha,\n",
        "        l1_ratio=l1_ratio,\n",
        "        random_state=42\n",
        "    )\n",
        "    if init_W is not None:\n",
        "        H_init = np.abs(np.random.rand(n_components, data.shape[1])) + 1e-6\n",
        "        model.fit(data, W=init_W, H=H_init)\n",
        "    else:\n",
        "        model.fit(data)\n",
        "    return model\n",
        "\n",
        "# Function to evaluate metrics\n",
        "def evaluate_metrics(target, prediction, threshold=None):\n",
        "    observed_mask = target > 0\n",
        "    target_observed = target[observed_mask]\n",
        "    prediction_observed = prediction[observed_mask]\n",
        "\n",
        "    rmse_value = sqrt(mean_squared_error(target_observed, prediction_observed))\n",
        "    mae_value = mean_absolute_error(target_observed, prediction_observed)\n",
        "\n",
        "    if threshold is None:\n",
        "        threshold = np.mean(prediction_observed)\n",
        "\n",
        "    binary_target = (target_observed >= threshold).astype(int)\n",
        "    binary_prediction = (prediction_observed >= threshold).astype(int)\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
        "        binary_target, binary_prediction, average='binary', zero_division=0\n",
        "    )\n",
        "    return rmse_value, mae_value, precision, recall, f1_score\n",
        "\n",
        "# Function to filter sparse data\n",
        "def filter_sparse_data(ratings, min_user_ratings=5, min_item_ratings=5):\n",
        "    user_counts = np.sum(ratings > 0, axis=1)\n",
        "    user_filter = user_counts >= min_user_ratings\n",
        "    ratings_filtered = ratings[user_filter, :]\n",
        "\n",
        "    item_counts = np.sum(ratings_filtered > 0, axis=0)\n",
        "    item_filter = item_counts >= min_item_ratings\n",
        "    ratings_filtered = ratings_filtered[:, item_filter]\n",
        "\n",
        "    return ratings_filtered\n"
      ],
      "metadata": {
        "id": "A2XHq7J0ZZVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}